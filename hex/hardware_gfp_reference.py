#!/usr/bin/env python3
"""
Hardware-Accurate GFP GEMM Reference Model

PURPOSE:
--------
This script generates golden references for GFP GEMM hardware verification.
It produces both hardware-accurate and emulator-based golden files for comparison.

OUTPUTS:
--------
- golden_B{B}_C{C}_V{V}.hex: Hardware-accurate reference (matches RTL)
- golden_B{B}_C{C}_V{V}_emu_NEW.hex: Emulator-based reference
- out.hex: Same as hardware-accurate reference
- Comprehensive comparison metrics (RMSE, max error, match rate)

HARDWARE ALGORITHM (from compute_engine.sv):
--------------------------------------------
For each output element C[i,j]:
  1. Compute 4 group dot products (integer multiply-accumulate)
  2. Align mantissas by maximum exponent across groups
  3. Sum aligned mantissas as integers
  4. Convert GFP result to FP16 with IEEE 754 rounding

EMULATOR ALGORITHM:
-------------------
For each output element C[i,j]:
  1. Collect V dot products (one per Native Vector)
  2. Find global maximum exponent across all V results
  3. Align all mantissas to max exponent
  4. Sum aligned mantissas
  5. Convert to FP16

KEY DIFFERENCE:
---------------
Hardware: Group-by-group processing with incremental accumulation
Emulator: Batch alignment across all V dot products (global max-align-sum)
Both are mathematically correct but produce tiny FP16 rounding differences
in high-V configurations.

USAGE:
------
  python hardware_gfp_reference.py --B 4 --C 4 --V 4

Loads matrices from left.hex and right.hex (generated by generate_nv_hex.py).
"""

import numpy as np
import sys
import struct
import os

# Add hex utilities from local directory
script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()
sys.path.insert(0, script_dir)
from mem_layout import load_hex_file, decode_exponents, decode_mantissas

# Try to import GFP emulator library (optional)
try:
    import torch
    # Use local emulator (same as mem_layout.py)
    emulator_path = os.path.join(os.path.dirname(__file__), '..', 'emulator', 'src', 'emulator')
    if emulator_path not in sys.path:
        sys.path.insert(0, emulator_path)
    from group_floating_point import GFPTensor, GFPDataType, GFPGemm
    GFP_EMULATOR_AVAILABLE = True
except ImportError as e:
    GFP_EMULATOR_AVAILABLE = False
    print(f"WARNING: GFP emulator library not available: {e}")


class HardwareGFPCompute:
    """
    Hardware-accurate GFP computation matching compute_engine.sv behavior.
    """

    def __init__(self, exp_bits=5, exp_bias=15, group_size=32):
        """
        Initialize hardware GFP parameters.

        Args:
            exp_bits: Number of exponent bits (5 for GFP8)
            exp_bias: Exponent bias (15 for 5-bit exponent)
            group_size: Elements per group (32 for GFP8)
        """
        self.exp_bits = exp_bits
        self.exp_bias = exp_bias
        self.group_size = group_size
        self.num_groups = 4  # For 128-element vectors (128 / 32 = 4)

    def compute_dot_product(self, left_mant, left_exp, right_mant, right_exp):
        """
        Compute dot product of two GFP vectors using hardware algorithm.

        Matches gfp8_nv_dot.sv exponent-aligned integer summation algorithm.

        Args:
            left_mant: [128] array of left mantissas (int8)
            left_exp: [4] array of left exponents (uint5)
            right_mant: [128] array of right mantissas (int8)
            right_exp: [4] array of right exponents (uint5)

        Returns:
            tuple: (mantissa, exponent) GFP result before FP16 conversion
        """
        group_mantissas = []
        group_exponents = []

        # Step 1: Compute each group dot product (matching gfp8_group_dot.sv)
        for g in range(self.num_groups):
            # Integer accumulation within group
            accumulator = 0
            for i in range(self.group_size):
                elem_idx = g * self.group_size + i
                accumulator += int(left_mant[elem_idx]) * int(right_mant[elem_idx])

            # Calculate exponent: exp_result = exp_left + exp_right - 2*bias
            exp_sum = int(left_exp[g]) + int(right_exp[g]) - 2 * self.exp_bias
            
            group_mantissas.append(accumulator)
            group_exponents.append(exp_sum)

        # Step 2: Exponent alignment and integer summation (matching gfp8_nv_dot.sv lines 116-134)
        # Find maximum exponent among all 4 groups
        max_exponent = max(group_exponents)
        
        # Align mantissas by right-shifting to max exponent
        aligned_mantissas = []
        for g in range(self.num_groups):
            exp_diff = max_exponent - group_exponents[g]
            
            # Check for underflow (matching gfp8_nv_dot.sv line 129-131)
            if exp_diff > 31:
                # Underflow - set to zero
                aligned = 0
            else:
                # Python's >> operator performs arithmetic right shift for negative numbers
                # (same behavior as SystemVerilog's >>> operator)
                aligned = group_mantissas[g] >> exp_diff
            aligned_mantissas.append(aligned)
        
        # Sum aligned mantissas as integers
        sum_mantissa = sum(aligned_mantissas)
        
        # Return GFP result (mantissa, exponent) - do NOT convert to float yet!
        # Hardware returns this as GFP, only converts to FP16 after V-loop accumulation
        return (sum_mantissa, max_exponent)

    def gfp_add(self, gfp1, gfp2):
        """
        Add two GFP values using exponent-aligned integer addition.
        
        Matches hardware gfp8_bcv_controller.sv accumulation logic.
        
        Args:
            gfp1: (mantissa1, exponent1) tuple
            gfp2: (mantissa2, exponent2) tuple
        
        Returns:
            (mantissa_sum, exponent_max) tuple
        """
        mant1, exp1 = gfp1
        mant2, exp2 = gfp2
        
        # Find maximum exponent
        max_exp = max(exp1, exp2)
        
        # Align mantissas to max exponent
        exp_diff1 = max_exp - exp1
        exp_diff2 = max_exp - exp2
        
        # Right shift with underflow check
        # Python's >> operator performs arithmetic right shift (same as SystemVerilog >>>)
        if exp_diff1 > 31:
            aligned_mant1 = 0
        else:
            aligned_mant1 = mant1 >> exp_diff1

        if exp_diff2 > 31:
            aligned_mant2 = 0
        else:
            aligned_mant2 = mant2 >> exp_diff2
        
        # Integer addition of aligned mantissas
        sum_mant = aligned_mant1 + aligned_mant2
        
        return (sum_mant, max_exp)

    def gfp_to_float(self, gfp):
        """
        Convert GFP (mantissa, exponent) to floating-point value.
        
        Args:
            gfp: (mantissa, exponent) tuple
        
        Returns:
            float: Real value
        """
        mantissa, exponent = gfp
        scale_factor = 2.0 ** exponent
        return float(mantissa) * scale_factor

    def gfp_to_fp16(self, gfp):
        """
        Convert GFP (mantissa, exponent) directly to FP16 format.
        
        Matches gfp8_to_fp16.sv hardware behavior exactly.
        
        Args:
            gfp: (mantissa, exponent) tuple where mantissa is signed 32-bit int
        
        Returns:
            uint16: FP16 representation
        """
        mantissa, exponent = gfp
        
        # Handle zero case
        if mantissa == 0:
            return 0x0000
        
        # Extract sign
        sign = 1 if mantissa < 0 else 0
        abs_mantissa = abs(mantissa)
        
        # Find leading zeros (matching hardware casez logic exactly)
        # Hardware counts from MSB down, looking for first 1 bit
        if abs_mantissa == 0:
            leading_zeros = 32
        else:
            # Count leading zeros by finding position of MSB
            leading_zeros = 0
            for i in range(31, -1, -1):
                if (abs_mantissa >> i) & 1:
                    break
                leading_zeros += 1
        
        # Normalize mantissa by shifting left
        normalized_mantissa = abs_mantissa << leading_zeros
        
        # Calculate FP16 exponent
        # Formula: fp16_exp_signed = exp_signed + 31 - leading_zeros + 15
        fp16_exp_signed = int(exponent) + 31 - leading_zeros + 15
        
        # Handle exponent range
        if fp16_exp_signed < -10:
            # Underflow to zero
            return 0x0000 | (sign << 15)
        elif fp16_exp_signed < 1:
            # Denormal range
            denorm_shift = 1 - fp16_exp_signed
            denorm_mantissa = abs_mantissa >> denorm_shift
            
            # Extract mantissa and rounding bits
            mant_truncated = (denorm_mantissa >> 21) & 0x3FF
            round_bit = (denorm_mantissa >> 20) & 1
            sticky_bit = (denorm_mantissa & 0xFFFFF) != 0
            
            # IEEE 754 round-to-nearest-even
            if round_bit and (sticky_bit or (mant_truncated & 1)):
                mant_rounded = mant_truncated + 1
                fp16_mant = mant_rounded & 0x3FF
            else:
                fp16_mant = mant_truncated
            
            return (sign << 15) | fp16_mant
        elif fp16_exp_signed > 30:
            # Overflow to infinity
            return (sign << 15) | 0x7C00
        else:
            # Normal case
            fp16_exp = fp16_exp_signed
            
            # Extract mantissa and rounding bits from normalized mantissa
            mant_truncated = (normalized_mantissa >> 21) & 0x3FF
            round_bit = (normalized_mantissa >> 20) & 1
            sticky_bit = (normalized_mantissa & 0xFFFFF) != 0
            
            # IEEE 754 round-to-nearest-even
            if round_bit and (sticky_bit or (mant_truncated & 1)):
                mant_rounded = mant_truncated + 1
                if mant_rounded >= 1024:  # Overflow into exponent
                    fp16_exp += 1
                    fp16_mant = 0
                    if fp16_exp >= 31:
                        return (sign << 15) | 0x7C00
                else:
                    fp16_mant = mant_rounded
            else:
                fp16_mant = mant_truncated
            
            return (sign << 15) | (fp16_exp << 10) | fp16_mant

    def real_to_fp16(self, val):
        """
        Convert real value to FP16 using IEEE 754 round-to-nearest-even.

        Uses numpy's float16 conversion which implements IEEE 754 rounding correctly.

        Args:
            val: Float value to convert

        Returns:
            uint16: FP16 representation
        """
        # Clamp to FP16 range (matching hardware's clamp at line 553-559)
        FP16_MAX = 65504.0
        if val > FP16_MAX:
            clamped_val = FP16_MAX
        elif val < -FP16_MAX:
            clamped_val = -FP16_MAX
        else:
            clamped_val = val
        
        # Use numpy's float16 conversion which matches IEEE 754 behavior
        # Hardware uses IEEE 754 round-to-nearest-even, which numpy implements
        fp16_val = np.float16(clamped_val)
        fp16_bits = fp16_val.view(np.uint16)
        
        return fp16_bits

    def compute_gemm(self, left_mant, left_exp, right_mant, right_exp,
                     output_rows=128, output_cols=128):
        """
        Compute full GEMM using hardware algorithm.

        Hardware computes in 4×4 tiles, processing in tile-major order.
        Each tile computes 16 dot products.

        Args:
            left_mant: [128, 128] left mantissa matrix
            left_exp: [128, 4] left exponent matrix
            right_mant: [128, 128] right mantissa matrix (transposed)
            right_exp: [128, 4] right exponent matrix (transposed)
            output_rows: Number of output rows (default 128)
            output_cols: Number of output columns (default 128)

        Returns:
            [16384] array of FP16 results in tile-major order
        """
        results_tile_major = []

        # Process in tile-major order: tile(0,0), tile(0,1), ..., tile(31,31)
        # Addressing matches hardware testbench (tb_ucode_gen.sv):
        # - left_addr = row_block * 16 (base address, hardware adds +16 for mantissas)
        # - right_addr = 528 + col_block * 16 (base address, hardware adds +16 for mantissas)
        # Python directly indexes matrices, so we just use row/col indices

        num_tile_rows = output_rows // 4
        num_tile_cols = output_cols // 4

        for tile_row in range(num_tile_rows):
            for tile_col in range(num_tile_cols):
                # Compute 4×4 tile
                for row_in_tile in range(4):
                    for col_in_tile in range(4):
                        # Output matrix position
                        out_row = tile_row * 4 + row_in_tile
                        out_col = tile_col * 4 + col_in_tile

                        # Extract vectors for this dot product
                        # Left: row from left matrix (A[out_row, :])
                        left_vec_mant = left_mant[out_row, :]
                        left_vec_exp = left_exp[out_row, :]

                        # Right: row from right matrix (B^T[out_col, :] = B[:, out_col])
                        right_vec_mant = right_mant[out_col, :]
                        right_vec_exp = right_exp[out_col, :]

                        # Compute dot product using hardware algorithm
                        gfp_result = self.compute_dot_product(
                            left_vec_mant, left_vec_exp,
                            right_vec_mant, right_vec_exp
                        )

                        # Convert GFP to float then to FP16
                        dot_result = self.gfp_to_float(gfp_result)
                        fp16_result = self.real_to_fp16(dot_result)
                        results_tile_major.append(fp16_result)

                # Progress indicator
                tile_num = tile_row * num_tile_cols + tile_col
                if (tile_num + 1) % 64 == 0:
                    print(f"Progress: {tile_num + 1}/{num_tile_rows * num_tile_cols} tiles completed")

        return np.array(results_tile_major, dtype=np.uint16)

    def compute_gemm_with_bcv(self, left_mant, left_exp, right_mant, right_exp, B, C, V, left_start_addr=0, right_start_addr=0):
        """
        Compute GEMM with B, C, V parameters and V-loop accumulation.

        Uses hardware algorithm for group dot products, then applies global
        max-exponent alignment for V-loop accumulation:
        1. Compute V dot products (one per Native Vector) using compute_dot_product()
        2. Find global maximum exponent across all V results
        3. Align all mantissas to max exponent
        4. Sum aligned mantissas as integers
        5. Convert final GFP result to FP16

        Args:
            left_mant: [128, 128] left mantissa matrix
            left_exp: [128, 4] left exponent matrix
            right_mant: [128, 128] right mantissa matrix (transposed)
            right_exp: [128, 4] right exponent matrix (transposed)
            B: Output rows (batch size)
            C: Output columns
            V: Inner dimension multiplier (V Native Vectors per output)
            left_start_addr: Starting BRAM address for left matrix (in 4-line units)
            right_start_addr: Starting BRAM address for right matrix (in 4-line units)

        Returns:
            [B*C] array of FP16 results
        """
        results = []

        print(f"   Computing with V-loop accumulation:")
        print(f"   - For each of {B}×{C} outputs: collect {V} dot products")
        print(f"   - Apply global max-exponent alignment and sum")
        print(f"   - Total: {B*C*V} dot products")

        # Iterate over output positions
        for b in range(B):
            for c in range(C):
                # V-loop: Collect all dot products first (matching emulator)
                v_dot_products = []  # List of (mantissa, exponent) tuples
                
                for v in range(V):
                    # Calculate Native Vector indices with address offset
                    # Address is in 4-line units, so nv_idx = addr / 4
                    left_nv_idx = (left_start_addr // 4) + b * V + v
                    right_nv_idx = (right_start_addr // 4) + c * V + v
                    
                    # Extract vectors for this dot product
                    left_vec_mant = left_mant[left_nv_idx, :]
                    left_vec_exp = left_exp[left_nv_idx, :]
                    right_vec_mant = right_mant[right_nv_idx, :]
                    right_vec_exp = right_exp[right_nv_idx, :]
                    
                    # Compute single NV dot product (returns GFP)
                    dot_gfp = self.compute_dot_product(
                        left_vec_mant, left_vec_exp,
                        right_vec_mant, right_vec_exp
                    )
                    v_dot_products.append(dot_gfp)

                # V-loop accumulation: Global max-align-sum
                # Find maximum exponent across all V dot products
                max_exponent = max(dot[1] for dot in v_dot_products)
                
                # Align all mantissas to max exponent and sum
                aligned_sum = 0
                for mant, exp in v_dot_products:
                    exp_diff = max_exponent - exp
                    if exp_diff > 31:
                        aligned_mant = 0
                    else:
                        aligned_mant = mant >> exp_diff
                    aligned_sum += aligned_mant
                
                # Result is (aligned_sum, max_exponent)
                accumulator_gfp = (aligned_sum, max_exponent)
                
                # Convert GFP to float then to FP16 (matching emulator behavior)
                accumulator_float = self.gfp_to_float(accumulator_gfp)
                fp16_result = self.real_to_fp16(accumulator_float)
                results.append(fp16_result)
                
                # Debug first output
                if b == 0 and c == 0:
                    print(f"   Output[{b},{c}]: {V} dots, max_exp={max_exponent}")
                    print(f"                acc_gfp=({accumulator_gfp[0]}, {accumulator_gfp[1]})")
                    print(f"                     = {accumulator_float:.6f} -> FP16=0x{fp16_result:04x}")
        
        return np.array(results, dtype=np.uint16)

def generate_multitile_golden_reference(B, C, V, output_prefix="golden"):
    """
    Generate multi-tile golden reference with command sequence.
    
    Args:
        B: Output rows (batch size)
        C: Output columns  
        V: Inner dimension multiplier (V Native Vectors per output)
        output_prefix: Prefix for output files
    
    Returns:
        tuple: (flat_results, command_sequence)
    """
    import os
    
    # Calculate tile dimensions
    num_left_tile = 128 // (B * V)
    num_right_tile = 128 // (C * V)
    total_tiles = num_left_tile * num_right_tile
    total_results = total_tiles * B * C
    
    print(f"Multi-tile configuration:")
    print(f"  B={B}, C={C}, V={V}")
    print(f"  num_left_tile={num_left_tile}, num_right_tile={num_right_tile}")
    print(f"  total_tiles={total_tiles}, total_results={total_results}")
    
    # Load matrices from hex files
    script_dir = os.path.dirname(os.path.abspath(__file__))
    left_hex_path = os.path.join(script_dir, 'left.hex')
    right_hex_path = os.path.join(script_dir, 'right.hex')
    
    exp_left_raw, man_left_raw = load_hex_file(left_hex_path)
    left_exp_torch = decode_exponents(exp_left_raw)
    left_exp = left_exp_torch.numpy()
    left_mant = decode_mantissas(man_left_raw)

    exp_right_raw, man_right_raw = load_hex_file(right_hex_path)
    right_exp_torch = decode_exponents(exp_right_raw)
    right_exp = right_exp_torch.numpy()
    right_mant = decode_mantissas(man_right_raw)
    
    # Initialize hardware-accurate compute engine
    hw_compute = HardwareGFPCompute(exp_bits=5, exp_bias=15, group_size=32)
    
    # Generate results for all tiles
    flat_results = []
    command_sequence = []
    
    print(f"\nGenerating {total_tiles} tiles...")
    
    for left_tile_idx in range(num_left_tile):
        for right_tile_idx in range(num_right_tile):
            tile_num = left_tile_idx * num_right_tile + right_tile_idx
            
            # Calculate BRAM addresses using hardware formula
            # Each Native Vector takes 4 lines, so addr = nv_idx * 4
            left_addr = (left_tile_idx * B * V) * 4
            right_addr = (right_tile_idx * C * V) * 4
            
            # Compute B×C results for this tile
            tile_results = hw_compute.compute_gemm_with_bcv(
                left_mant, left_exp, right_mant, right_exp, 
                B, C, V, left_addr, right_addr
            )
            
            # Append to flat results array
            flat_results.extend(tile_results)
            
            # Add to command sequence
            command_sequence.append({
                'tile_num': tile_num,
                'left_tile_idx': left_tile_idx,
                'right_tile_idx': right_tile_idx,
                'left_addr': left_addr,
                'right_addr': right_addr,
                'results': tile_results.copy()
            })
            
            if (tile_num + 1) % 4 == 0:
                print(f"  Completed {tile_num + 1}/{total_tiles} tiles")
    
    # Write output files
    hex_filename = f"{output_prefix}_B{B}_C{C}_V{V}_multitile.hex"
    cmd_filename = f"{output_prefix}_commands_b{B}_c{C}_v{V}_multitile.txt"
    
    # Write hex file (flat results)
    with open(hex_filename, 'w') as f:
        for val in flat_results:
            f.write(f"{val:04x}\n")
    
    # Write command sequence file
    with open(cmd_filename, 'w') as f:
        f.write(f"# Multi-tile command sequence for B={B}, C={C}, V={V}\n")
        f.write(f"# num_left_tile={num_left_tile}, num_right_tile={num_right_tile}\n")
        f.write(f"# total_tiles={total_tiles}, total_results={total_results}\n\n")
        
        for cmd in command_sequence:
            f.write(f"Tile {cmd['tile_num']} (left_tile={cmd['left_tile_idx']}, right_tile={cmd['right_tile_idx']}):\n")
            f.write(f"  left_addr = ({cmd['left_tile_idx']}*{B}*{V})*4 = {cmd['left_addr']}\n")
            f.write(f"  right_addr = ({cmd['right_tile_idx']}*{C}*{V})*4 = {cmd['right_addr']}\n")
            f.write(f"  Expected: {len(cmd['results'])} results (B*C)\n")
            f.write(f"  Results: {' '.join(f'0x{v:04x}' for v in cmd['results'])}\n\n")
    
    print(f"\nGenerated files:")
    print(f"  {hex_filename}: {len(flat_results)} FP16 results")
    print(f"  {cmd_filename}: Command sequence with addresses")
    
    return flat_results, command_sequence


def compute_emulator_golden(B, C, V, left_mant, left_exp, right_mant, right_exp):
    """
    Generate emulator-based golden reference using loaded hex matrices.

    This function uses the same matrices as hardware path but applies the
    emulator's GFP GEMM algorithm for comparison.

    Args:
        B: Output rows (batch size)
        C: Output columns
        V: Inner dimension multiplier (V Native Vectors)
        left_mant: [128, 128] left mantissa matrix (from hex)
        left_exp: [128, 4] left exponent matrix (from hex)
        right_mant: [128, 128] right mantissa matrix (from hex)
        right_exp: [128, 4] right exponent matrix (from hex)

    Returns:
        np.array: FP16 results as uint16 array
    """
    if not GFP_EMULATOR_AVAILABLE:
        print("ERROR: GFP emulator library not available!")
        return None

    print(f"\n   Computing emulator-based golden reference...")
    print(f"   Loading matrices from hex files (same as hardware path)")

    # Convert GFP hex data to float tensors
    def gfp_to_float_matrix(mant_matrix, exp_matrix, bias=15):
        """Convert GFP (mantissa, exponent) to float matrix."""
        rows, cols = mant_matrix.shape
        float_matrix = np.zeros((rows, cols), dtype=np.float32)
        for i in range(rows):
            for g in range(4):  # 4 groups per row
                group_exp = exp_matrix[i, g]
                scale = 2.0 ** (int(group_exp) - bias)
                for j in range(32):  # 32 elements per group
                    col_idx = g * 32 + j
                    float_matrix[i, col_idx] = float(mant_matrix[i, col_idx]) * scale
        return float_matrix

    # Convert full 128 NV matrices to float
    left_float_full = gfp_to_float_matrix(left_mant, left_exp)
    right_float_full = gfp_to_float_matrix(right_mant, right_exp)

    # Extract submatrices for B×C×V configuration
    left_float = left_float_full[:B*V, :]  # Shape: (B*V) × 128
    right_float = right_float_full[:C*V, :]  # Shape: (C*V) × 128

    # Convert to torch tensors
    left_tensor = torch.from_numpy(left_float).float()
    right_tensor = torch.from_numpy(right_float).float()

    # Create GFP tensors
    gfp_dtype = GFPDataType(
        mantissa_bits=8,
        exp_bits=5,
        exp_bias=15,
        mantissa_signed=True
    )

    gfp_a = GFPTensor(
        original_shape=left_tensor.shape,
        group_axis=1,  # Matrix A: group along columns
        group_size=32,
        dtype=gfp_dtype,
        original_data=left_tensor
    )

    # For Matrix B, we need to transpose since it's stored transposed in hex
    right_tensor_transposed = right_tensor.T  # Shape: 128 × (C*V)
    gfp_b = GFPTensor(
        original_shape=right_tensor_transposed.shape,
        group_axis=0,  # Matrix B: group along rows
        group_size=32,
        dtype=gfp_dtype,
        original_data=right_tensor_transposed
    )

    # Debug: Check dequantized input values
    deq_a = gfp_a.dequantize()
    deq_b = gfp_b.dequantize()
    print(f"   Dequantized Matrix A range: [{deq_a.min():.6f}, {deq_a.max():.6f}]")
    print(f"   Dequantized Matrix B range: [{deq_b.min():.6f}, {deq_b.max():.6f}]")

    # Define GEMM data types (matching generate_nv_hex.py lines 266-275)
    product_dtype = GFPDataType(
        mantissa_bits=19,
        exp_bits=8,
        mantissa_signed=True
    )
    accum_dtype = GFPDataType(
        mantissa_bits=20,
        exp_bits=9,
        mantissa_signed=True
    )

    # Run GFP GEMM (matching generate_nv_hex.py lines 277-279)
    # This computes (B*V) × (C*V) GEMM
    gemm = GFPGemm(accum_dtype=accum_dtype, product_dtype=product_dtype)
    gfp_result = gemm(gfp_a, gfp_b)
    result_full = gfp_result.dequantize().cpu().numpy()  # Shape: (B*V) × (C*V)

    print(f"   Full GEMM result shape: {result_full.shape}")
    print(f"   Result range: [{result_full.min():.6f}, {result_full.max():.6f}]")

    # V-loop accumulation: sum over V dimension
    # result[b,c] = sum over v of result_full[b*V+v, c*V+v]
    result_accumulated = np.zeros((B, C), dtype=np.float32)
    for b in range(B):
        for c in range(C):
            for v in range(V):
                result_accumulated[b, c] += result_full[b*V + v, c*V + v]

    print(f"   Accumulated result shape: {result_accumulated.shape}")

    # Convert to FP16 hex (matching generate_nv_hex.py lines 298-303)
    result_fp16 = result_accumulated.astype(np.float16).flatten()
    results_emulator = np.array([np.frombuffer(val.tobytes(), dtype=np.uint16)[0] for val in result_fp16], dtype=np.uint16)

    print(f"   ✓ Generated {len(results_emulator)} FP16 results ({B}×{C})")
    print(f"   First 4 values: {' '.join(f'0x{v:04x}' for v in results_emulator[:min(4, len(results_emulator))])}")

    return results_emulator


def main():
    import argparse
    
    # Parse command-line arguments
    parser = argparse.ArgumentParser(description='Generate hardware-accurate GFP GEMM golden reference')
    parser.add_argument('--B', type=int, default=128, help='Output rows (batch size)')
    parser.add_argument('--C', type=int, default=128, help='Output columns')
    parser.add_argument('--V', type=int, default=1, help='Inner dimension multiplier (V Native Vectors)')
    parser.add_argument('--multitile', action='store_true', help='Generate multi-tile golden reference')
    parser.add_argument('--seed', type=int, default=1234, help='Random seed (must match generate_nv_hex.py)')
    parser.add_argument('--std', type=float, default=0.002, help='Standard deviation (must match generate_nv_hex.py)')
    args = parser.parse_args()
    
    B, C, V = args.B, args.C, args.V
    
    # Validate parameters
    if B * V > 128 or C * V > 128:
        print(f"ERROR: Invalid parameters! B×V={B*V} and C×V={C*V} must both be ≤ 128")
        sys.exit(1)
    
    # Handle multi-tile mode
    if args.multitile:
        print("=" * 80)
        print("Multi-Tile Golden Reference Generation")
        print("=" * 80)
        
        # Generate multi-tile golden reference
        flat_results, command_sequence = generate_multitile_golden_reference(B, C, V)
        
        print("\n" + "=" * 80)
        print("Multi-tile golden reference generation complete!")
        print("Files generated:")
        print(f"  golden_B{B}_C{C}_V{V}_multitile.hex - Flat FP16 results")
        print(f"  golden_commands_b{B}_c{C}_v{V}_multitile.txt - Command sequence")
        print("=" * 80)
        return
    
    # Single-tile mode (original behavior)
    print("=" * 80)
    print("Hardware-Accurate GFP GEMM Reference Generation")
    print("=" * 80)
    print(f"\nConfiguration: B={B}, C={C}, V={V}")
    print(f"  Matrix A: {B} × {128*V} (uses {B*V} Native Vectors)")
    print(f"  Matrix B: {128*V} × {C} (uses {C*V} Native Vectors)")
    print(f"  Output:   {B} × {C} = {B*C} results")

    # Load matrices from hex files
    print("\n1. Loading matrices from hex files...")
    left_hex_path = os.path.join(script_dir, 'left.hex')
    right_hex_path = os.path.join(script_dir, 'right.hex')
    
    exp_left_raw, man_left_raw = load_hex_file(left_hex_path)
    left_exp_torch = decode_exponents(exp_left_raw)      # [128, 4] torch.Tensor
    left_exp = left_exp_torch.numpy()                     # Convert to numpy for processing
    left_mant = decode_mantissas(man_left_raw)           # [128, 128] numpy array

    exp_right_raw, man_right_raw = load_hex_file(right_hex_path)
    right_exp_torch = decode_exponents(exp_right_raw)    # [128, 4] torch.Tensor
    right_exp = right_exp_torch.numpy()                   # Convert to numpy for processing
    right_mant = decode_mantissas(man_right_raw)         # [128, 128] numpy array (already transposed)

    print(f"   Left matrix: {left_mant.shape}, exponents: {left_exp.shape}")
    print(f"   Right matrix: {right_mant.shape}, exponents: {right_exp.shape}")
    print(f"   Mantissa range: [{np.min(left_mant)}, {np.max(left_mant)}]")
    print(f"   Exponent range: [{np.min(left_exp)}, {np.max(left_exp)}]")

    # Compute using selected method(s)
    results_hardware = None
    results_emulator = None
    
    # Initialize hardware-accurate compute engine
    print("\n2. Computing with Hardware-Accurate Algorithm...")
    hw_compute = HardwareGFPCompute(exp_bits=5, exp_bias=15, group_size=32)
    print(f"   Computing {B}×{C} GEMM with V={V} accumulation...")
    results_hardware = hw_compute.compute_gemm_with_bcv(left_mant, left_exp, right_mant, right_exp, B, C, V)
    print(f"   ✓ Generated {len(results_hardware)} FP16 results ({B}×{C})")
    print(f"   First 4 values: {' '.join(f'0x{v:04x}' for v in results_hardware[:4])}")
    
    # Compute using GFP emulator for comparison (new simplified implementation)
    print("\n3. Computing with GFP Emulator (same matrices, emulator algorithm)...")
    results_emulator = compute_emulator_golden(B, C, V, left_mant, left_exp, right_mant, right_exp)
    
    # Compare hardware vs emulator results with comprehensive metrics
    if results_hardware is not None and results_emulator is not None:
        print("\n" + "=" * 80)
        print(f"Hardware vs Emulator Comparison (B={B}, C={C}, V={V})")
        print("=" * 80)

        # Convert to float for numerical comparison
        hw_float = np.array([np.frombuffer(struct.pack('<H', v), dtype=np.float16)[0] for v in results_hardware], dtype=np.float32)
        emu_float = np.array([np.frombuffer(struct.pack('<H', v), dtype=np.float16)[0] for v in results_emulator], dtype=np.float32)

        # Exact FP16 match count
        total = len(results_hardware)
        exact_matches = np.sum(results_hardware == results_emulator)
        match_rate = 100.0 * exact_matches / total

        print(f"Total results: {total} ({B}×{C})")
        print(f"Exact FP16 matches: {exact_matches}/{total} ({match_rate:.1f}%)")

        # Compute error metrics
        abs_errors = np.abs(hw_float - emu_float)
        rmse = np.sqrt(np.mean((hw_float - emu_float)**2))
        max_error = np.max(abs_errors)
        mean_error = np.mean(abs_errors)

        print(f"\nNumerical Error Analysis:")
        print(f"  RMSE:               {rmse:.6e}")
        print(f"  Max absolute error: {max_error:.6e}")
        print(f"  Mean absolute error: {mean_error:.6e}")

        # Show detailed mismatches
        if exact_matches < total:
            print(f"\nFirst 10 mismatches:")
            mismatch_count = 0
            for i in range(total):
                if results_hardware[i] != results_emulator[i]:
                    hw_val = hw_float[i]
                    emu_val = emu_float[i]
                    diff = abs(hw_val - emu_val)
                    rel_err = (diff / abs(emu_val) * 100) if emu_val != 0 else 0
                    print(f"  [{i:3d}] HW=0x{results_hardware[i]:04x} ({hw_val:10.6f})  "
                          f"EMU=0x{results_emulator[i]:04x} ({emu_val:10.6f})  "
                          f"diff={diff:.6e} rel_err={rel_err:.2f}%")
                    mismatch_count += 1
                    if mismatch_count >= 10:
                        break
        else:
            print("\n✓ Perfect match between hardware and emulator!")

        print("=" * 80)
    
    # Write to output files (use hardware results by default, or emulator if that's all we have)
    results_to_write = results_hardware if results_hardware is not None else results_emulator

    # Write emulator results to _NEW file for validation
    if results_emulator is not None:
        emu_golden_path_new = os.path.join(script_dir, f'golden_B{B}_C{C}_V{V}_emu_NEW.hex')
        print(f"\n   Writing NEW emulator results to {emu_golden_path_new}...")
        with open(emu_golden_path_new, 'w') as f:
            for val in results_emulator:
                f.write(f"{val:04x}\n")
        print(f"   ✓ Wrote {len(results_emulator)} values")
        print(f"   NOTE: Compare this with golden_B{B}_C{C}_V{V}_emu.hex from generate_nv_hex.py")
    
    if results_to_write is not None:
        output_path = os.path.join(script_dir, 'out.hex')
        golden_path = os.path.join(script_dir, f'golden_B{B}_C{C}_V{V}.hex')
        
        step_num = 5
        print(f"\n{step_num}. Writing results...")
        print(f"   Writing to {output_path}...")
        with open(output_path, 'w') as f:
            for val in results_to_write:
                f.write(f"{val:04x}\n")
        print(f"   ✓ Wrote {len(results_to_write)} values")
        
        print(f"   Writing to {golden_path}...")
        with open(golden_path, 'w') as f:
            for val in results_to_write:
                f.write(f"{val:04x}\n")
        print(f"   ✓ Wrote {len(results_to_write)} values")

        # Verify first results
        print(f"\n{step_num+1}. Verification:")
        num_to_show = min(16, len(results_to_write))
        print(f"   First {num_to_show} results:")
        for i in range(num_to_show):
            row = i // C
            col = i % C
            fp16_bits = results_to_write[i]
            fp16_val = np.frombuffer(struct.pack('<H', fp16_bits), dtype=np.float16)[0]
            print(f"     [{i:2d}] output[{row},{col}] = 0x{fp16_bits:04x} ({fp16_val:8.4f})")

    print("\n" + "=" * 80)
    print("Golden reference generation complete!")
    print("Hardware-accurate GFP algorithm used:")
    print("  - Group-by-group integer accumulation")
    print("  - Exponent-aligned integer summation (matches gfp8_nv_dot.sv)")
    print("  - V-loop accumulation (matches gfp8_bcv_controller.sv)")
    print("  - FP16 conversion with overflow clamping")
    print("=" * 80)


if __name__ == '__main__':
    main()
