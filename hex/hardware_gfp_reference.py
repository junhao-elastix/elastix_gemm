#!/usr/bin/env python3
"""
Hardware-Accurate GFP GEMM Reference Model

PURPOSE:
--------
This script generates golden references for GFP GEMM hardware verification.
It produces both hardware-accurate and emulator-based golden files for comparison.

OUTPUTS:
--------
- golden_B{B}_C{C}_V{V}.hex: Hardware-accurate reference (matches RTL)
- golden_B{B}_C{C}_V{V}_emu_NEW.hex: Emulator-based reference
- out.hex: Same as hardware-accurate reference
- Comprehensive comparison metrics (RMSE, max error, match rate)

HARDWARE ALGORITHM (from compute_engine.sv):
--------------------------------------------
For each output element C[i,j]:
  1. Compute 4 group dot products (integer multiply-accumulate)
  2. Align mantissas by maximum exponent across groups
  3. Sum aligned mantissas as integers
  4. Convert GFP result to FP16 with IEEE 754 rounding

EMULATOR ALGORITHM:
-------------------
For each output element C[i,j]:
  1. Collect V dot products (one per Native Vector)
  2. Find global maximum exponent across all V results
  3. Align all mantissas to max exponent
  4. Sum aligned mantissas
  5. Convert to FP16

KEY DIFFERENCE:
---------------
Hardware: Group-by-group processing with incremental accumulation
Emulator: Batch alignment across all V dot products (global max-align-sum)
Both are mathematically correct but produce tiny FP16 rounding differences
in high-V configurations.

USAGE:
------
  python hardware_gfp_reference.py --B 4 --C 4 --V 4

Loads matrices from left.hex and right.hex (generated by generate_nv_hex.py).
"""

import numpy as np
import sys
import struct
import os
import argparse

# Add hex utilities from local directory
script_dir = os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd()
sys.path.insert(0, script_dir)
from mem_layout import load_hex_file, decode_exponents, decode_mantissas

# Try to import GFP emulator library (optional)
try:
    import torch
    # Use local emulator (same as mem_layout.py)
    emulator_path = os.path.join(os.path.dirname(__file__), '..', 'emulator', 'src', 'emulator')
    if emulator_path not in sys.path:
        sys.path.insert(0, emulator_path)
    from group_floating_point import GFPTensor, GFPDataType, GFPGemm
    GFP_EMULATOR_AVAILABLE = True
except ImportError as e:
    GFP_EMULATOR_AVAILABLE = False
    print(f"WARNING: GFP emulator library not available: {e}")


class HardwareGFPCompute:
    """
    Hardware-accurate GFP computation matching compute_engine.sv behavior.
    """

    def __init__(self, exp_bits=5, exp_bias=15, group_size=32):
        """
        Initialize hardware GFP parameters.

        Args:
            exp_bits: Number of exponent bits (5 for GFP8)
            exp_bias: Exponent bias (15 for 5-bit exponent)
            group_size: Elements per group (32 for GFP8)
        """
        self.exp_bits = exp_bits
        self.exp_bias = exp_bias
        self.group_size = group_size
        self.num_groups = 4  # For 128-element vectors (128 / 32 = 4)

    def compute_dot_product(self, left_mant, left_exp, right_mant, right_exp):
        """
        Compute dot product of two GFP vectors using hardware algorithm.

        Matches gfp8_nv_dot.sv exponent-aligned integer summation algorithm.

        Args:
            left_mant: [128] array of left mantissas (int8)
            left_exp: [4] array of left exponents (uint5)
            right_mant: [128] array of right mantissas (int8)
            right_exp: [4] array of right exponents (uint5)

        Returns:
            tuple: (mantissa, exponent) GFP result before FP16 conversion
        """
        group_mantissas = []
        group_exponents = []

        # Step 1: Compute each group dot product (matching gfp8_group_dot.sv)
        for g in range(self.num_groups):
            # Integer accumulation within group
            accumulator = 0
            for i in range(self.group_size):
                elem_idx = g * self.group_size + i
                accumulator += int(left_mant[elem_idx]) * int(right_mant[elem_idx])

            # Calculate exponent: exp_result = exp_left + exp_right - 2*bias
            exp_sum = int(left_exp[g]) + int(right_exp[g]) - 2 * self.exp_bias
            
            group_mantissas.append(accumulator)
            group_exponents.append(exp_sum)

        # Step 2: Exponent alignment and integer summation (matching gfp8_nv_dot.sv lines 116-134)
        # Find maximum exponent among all 4 groups
        max_exponent = max(group_exponents)
        
        # Align mantissas by right-shifting to max exponent
        aligned_mantissas = []
        for g in range(self.num_groups):
            exp_diff = max_exponent - group_exponents[g]
            
            # Check for underflow (matching gfp8_nv_dot.sv line 129-131)
            if exp_diff > 31:
                # Underflow - set to zero
                aligned = 0
            else:
                # Python's >> operator performs arithmetic right shift for negative numbers
                # (same behavior as SystemVerilog's >>> operator)
                aligned = group_mantissas[g] >> exp_diff
            aligned_mantissas.append(aligned)
        
        # Sum aligned mantissas as integers
        sum_mantissa = sum(aligned_mantissas)
        
        # Return GFP result (mantissa, exponent) - do NOT convert to float yet!
        # Hardware returns this as GFP, only converts to FP16 after V-loop accumulation
        return (sum_mantissa, max_exponent)

    def gfp_to_fp16(self, gfp):
        """
        Convert GFP (mantissa, exponent) to FP16 format.

        Converts GFP to float, clamps to FP16 range, then converts to FP16
        using IEEE 754 round-to-nearest-even.

        Args:
            gfp: (mantissa, exponent) tuple

        Returns:
            uint16: FP16 representation
        """
        mantissa, exponent = gfp

        # Convert GFP to float
        scale_factor = 2.0 ** exponent
        float_val = float(mantissa) * scale_factor

        # Clamp to FP16 range
        FP16_MAX = 65504.0
        if float_val > FP16_MAX:
            clamped_val = FP16_MAX
        elif float_val < -FP16_MAX:
            clamped_val = -FP16_MAX
        else:
            clamped_val = float_val

        # Convert to FP16 using numpy (IEEE 754 round-to-nearest-even)
        fp16_val = np.float16(clamped_val)
        fp16_bits = fp16_val.view(np.uint16)

        return fp16_bits

    def compute_gemm_with_bcv(self, left_mant, left_exp, right_mant, right_exp, B, C, V, left_start_addr=0, right_start_addr=0):
        """
        Compute GEMM with B, C, V parameters and V-loop accumulation.

        Uses hardware algorithm for group dot products, then applies global
        max-exponent alignment for V-loop accumulation:
        1. Compute V dot products (one per Native Vector) using compute_dot_product()
        2. Find global maximum exponent across all V results
        3. Align all mantissas to max exponent
        4. Sum aligned mantissas as integers
        5. Convert final GFP result to FP16

        Args:
            left_mant: [128, 128] left mantissa matrix
            left_exp: [128, 4] left exponent matrix
            right_mant: [128, 128] right mantissa matrix (transposed)
            right_exp: [128, 4] right exponent matrix (transposed)
            B: Output rows (batch size)
            C: Output columns
            V: Inner dimension multiplier (V Native Vectors per output)
            left_start_addr: Starting BRAM address for left matrix (in 4-line units)
            right_start_addr: Starting BRAM address for right matrix (in 4-line units)

        Returns:
            [B*C] array of FP16 results
        """
        results = []

        print(f"   Computing with V-loop accumulation:")
        print(f"   - For each of {B}x{C} outputs: collect {V} dot products")
        print(f"   - Apply global max-exponent alignment and sum")
        print(f"   - Total: {B*C*V} dot products")

        # Iterate over output positions
        for b in range(B):
            for c in range(C):
                # V-loop: Collect all dot products first
                v_dot_products = []  # List of (mantissa, exponent) tuples
                
                for v in range(V):
                    # Calculate Native Vector indices with address offset
                    # Address is in 4-line units, so nv_idx = addr / 4
                    left_nv_idx = (left_start_addr // 4) + b * V + v
                    right_nv_idx = (right_start_addr // 4) + c * V + v
                    
                    # Extract vectors for this dot product
                    left_vec_mant = left_mant[left_nv_idx, :]
                    left_vec_exp = left_exp[left_nv_idx, :]
                    right_vec_mant = right_mant[right_nv_idx, :]
                    right_vec_exp = right_exp[right_nv_idx, :]
                    
                    # Compute single NV dot product (returns GFP)
                    dot_gfp = self.compute_dot_product(
                        left_vec_mant, left_vec_exp,
                        right_vec_mant, right_vec_exp
                    )
                    v_dot_products.append(dot_gfp)

                # V-loop accumulation: Global max-align-sum
                # Find maximum exponent across all V dot products
                max_exponent = max(dot[1] for dot in v_dot_products)
                
                # Align all mantissas to max exponent and sum
                aligned_sum = 0
                for mant, exp in v_dot_products:
                    exp_diff = max_exponent - exp
                    if exp_diff > 31:
                        aligned_mant = 0
                    else:
                        aligned_mant = mant >> exp_diff
                    aligned_sum += aligned_mant
                
                # Result is (aligned_sum, max_exponent)
                accumulator_gfp = (aligned_sum, max_exponent)

                # Convert GFP to FP16
                fp16_result = self.gfp_to_fp16(accumulator_gfp)
                results.append(fp16_result)

                # Debug first output
                if b == 0 and c == 0:
                    print(f"   Output[{b},{c}]: {V} dots, max_exp={max_exponent}")
                    print(f"                acc_gfp=({accumulator_gfp[0]}, {accumulator_gfp[1]})")
                    print(f"                     -> FP16=0x{fp16_result:04x}")
        
        return np.array(results, dtype=np.uint16)

def generate_multitile_golden_reference(B, C, V, output_prefix="golden"):
    """
    Generate multi-tile golden reference with command sequence.

    This is a TEST CASE PATTERN that uses sequential tiling based on the
    bottleneck principle. In production, software can use ANY addressing
    pattern - hardware is completely address-agnostic.

    Args:
        B: Output rows (batch size)
        C: Output columns
        V: Inner dimension multiplier (V Native Vectors per output)
        output_prefix: Prefix for output files

    Returns:
        tuple: (flat_results, command_sequence)
    """
    # Calculate tile dimensions using BOTTLENECK PRINCIPLE
    # Given 128 Native Vectors total in reference matrices:
    max_left_tiles = 128 // (B * V)   # How many left chunks available
    max_right_tiles = 128 // (C * V)  # How many right chunks available

    # We can only compute as many tiles as the limiting side allows
    num_tiles = min(max_left_tiles, max_right_tiles)  # Bottleneck
    total_results = num_tiles * B * C

    print(f"Multi-tile configuration:")
    print(f"  B={B}, C={C}, V={V}")
    print(f"  max_left_tiles={max_left_tiles}, max_right_tiles={max_right_tiles}")
    print(f"  num_tiles={num_tiles} (bottleneck), total_results={total_results}")

    # Load matrices from hex files
    left_hex_path = os.path.join(script_dir, 'left.hex')
    right_hex_path = os.path.join(script_dir, 'right.hex')
    
    exp_left_raw, man_left_raw = load_hex_file(left_hex_path)
    left_exp_torch = decode_exponents(exp_left_raw)
    left_exp = left_exp_torch.numpy()
    left_mant = decode_mantissas(man_left_raw)

    exp_right_raw, man_right_raw = load_hex_file(right_hex_path)
    right_exp_torch = decode_exponents(exp_right_raw)
    right_exp = right_exp_torch.numpy()
    right_mant = decode_mantissas(man_right_raw)
    
    # Initialize hardware-accurate compute engine
    hw_compute = HardwareGFPCompute(exp_bits=5, exp_bias=15, group_size=32)
    
    # Generate results for all tiles
    # TEST CASE PATTERN: Sequential tiling with lockstep advancement
    # NOTE: In production, software can use ANY addressing pattern!
    flat_results = []
    command_sequence = []

    # Calculate strides for this sequential tiling pattern
    left_stride = (B * V) * 4   # Lines to advance for each tile
    right_stride = (C * V) * 4  # Lines to advance for each tile

    print(f"\nGenerating {num_tiles} tiles...")
    print(f"Address strides: left={left_stride} lines, right={right_stride} lines")

    for tile_idx in range(num_tiles):
        # For this test pattern: both sides advance together (lockstep)
        # In production: software could choose ANY addressing pattern!
        # (e.g., broadcast, sparse, reordered, arbitrary addresses, etc.)
        left_addr = tile_idx * left_stride
        right_addr = tile_idx * right_stride

        # Compute BxC results for this tile
        # Hardware only sees: start addresses, lengths, V-depth
        tile_results = hw_compute.compute_gemm_with_bcv(
            left_mant, left_exp, right_mant, right_exp,
            B, C, V, left_addr, right_addr
        )

        # Store results in issue order
        flat_results.extend(tile_results)

        # Add to command sequence
        command_sequence.append({
            'tile_idx': tile_idx,
            'left_addr': left_addr,
            'right_addr': right_addr,
            'left_stride': left_stride,
            'right_stride': right_stride,
            'results': tile_results.copy()
        })

        if (tile_idx + 1) % 4 == 0:
            print(f"  Completed {tile_idx + 1}/{num_tiles} tiles")

    # Hardware writes results sequentially when using TILE + WAIT_TILE pairs
    # Software orchestrates the addressing pattern via single-loop iteration
    
    # Write output files
    hex_filename = f"{output_prefix}_B{B}_C{C}_V{V}_multitile.hex"
    cmd_filename = f"{output_prefix}_B{B}_C{C}_V{V}_multitile_commands.txt"
    
    # Write hex file (flat results)
    with open(hex_filename, 'w') as f:
        for val in flat_results:
            f.write(f"{val:04x}\n")
    
    # Write command sequence file
    with open(cmd_filename, 'w') as f:
        f.write(f"# Multi-tile command sequence for B={B}, C={C}, V={V}\n")
        f.write(f"# TEST CASE PATTERN: Sequential tiling with lockstep advancement\n")
        f.write(f"# max_left_tiles={max_left_tiles}, max_right_tiles={max_right_tiles}\n")
        f.write(f"# num_tiles={num_tiles} (bottleneck), total_results={total_results}\n")
        f.write(f"# left_stride={left_stride} lines, right_stride={right_stride} lines\n")
        f.write(f"#\n")
        f.write(f"# NOTE: This is just our test pattern. In production, software can use\n")
        f.write(f"# ANY addressing pattern - hardware is completely address-agnostic!\n\n")

        for cmd in command_sequence:
            f.write(f"Tile {cmd['tile_idx']}:\n")
            f.write(f"  left_addr = {cmd['tile_idx']} × {cmd['left_stride']} = {cmd['left_addr']}\n")
            f.write(f"  right_addr = {cmd['tile_idx']} × {cmd['right_stride']} = {cmd['right_addr']}\n")
            f.write(f"  leftUgdLen = {B}, rightUgdLen = {C}, vecLen = {V}\n")
            f.write(f"  Expected: {len(cmd['results'])} results (B×C = {B}×{C})\n")
            f.write(f"  Results: {' '.join(f'0x{v:04x}' for v in cmd['results'])}\n\n")
    
    print(f"\nGenerated files:")
    print(f"  {hex_filename}: {len(flat_results)} FP16 results")
    print(f"  {cmd_filename}: Command sequence with addresses")
    
    return flat_results, command_sequence


def compute_emulator_golden(B, C, V, left_mant, left_exp, right_mant, right_exp):
    """
    Generate emulator-based golden reference using loaded hex matrices.

    This function uses the same matrices as hardware path but applies the
    emulator's GFP GEMM algorithm for comparison.

    Args:
        B: Output rows (batch size)
        C: Output columns
        V: Inner dimension multiplier (V Native Vectors)
        left_mant: [128, 128] left mantissa matrix (from hex)
        left_exp: [128, 4] left exponent matrix (from hex)
        right_mant: [128, 128] right mantissa matrix (from hex)
        right_exp: [128, 4] right exponent matrix (from hex)

    Returns:
        np.array: FP16 results as uint16 array
    """
    if not GFP_EMULATOR_AVAILABLE:
        print("ERROR: GFP emulator library not available!")
        return None

    print(f"\n   Computing emulator-based golden reference...")
    print(f"   Loading matrices from hex files (same as hardware path)")

    # Convert GFP hex data to float tensors
    def gfp_to_float_matrix(mant_matrix, exp_matrix, bias=15):
        """Convert GFP (mantissa, exponent) to float matrix."""
        rows, cols = mant_matrix.shape
        float_matrix = np.zeros((rows, cols), dtype=np.float32)
        for i in range(rows):
            for g in range(4):  # 4 groups per row
                group_exp = exp_matrix[i, g]
                scale = 2.0 ** (int(group_exp) - bias)
                for j in range(32):  # 32 elements per group
                    col_idx = g * 32 + j
                    float_matrix[i, col_idx] = float(mant_matrix[i, col_idx]) * scale
        return float_matrix

    # Convert full 128 NV matrices to float
    left_float_full = gfp_to_float_matrix(left_mant, left_exp)
    right_float_full = gfp_to_float_matrix(right_mant, right_exp)

    # Extract submatrices for BxCxV configuration
    left_float = left_float_full[:B*V, :]  # Shape: (B*V) x 128
    right_float = right_float_full[:C*V, :]  # Shape: (C*V) x 128

    # Convert to torch tensors
    left_tensor = torch.from_numpy(left_float).float()
    right_tensor = torch.from_numpy(right_float).float()

    # Create GFP tensors
    gfp_dtype = GFPDataType(
        mantissa_bits=8,
        exp_bits=5,
        exp_bias=15,
        mantissa_signed=True
    )

    gfp_a = GFPTensor(
        original_shape=left_tensor.shape,
        group_axis=1,  # Matrix A: group along columns
        group_size=32,
        dtype=gfp_dtype,
        original_data=left_tensor
    )

    # For Matrix B, we need to transpose since it's stored transposed in hex
    right_tensor_transposed = right_tensor.T  # Shape: 128 x (C*V)
    gfp_b = GFPTensor(
        original_shape=right_tensor_transposed.shape,
        group_axis=0,  # Matrix B: group along rows
        group_size=32,
        dtype=gfp_dtype,
        original_data=right_tensor_transposed
    )

    # Debug: Check dequantized input values
    deq_a = gfp_a.dequantize()
    deq_b = gfp_b.dequantize()
    print(f"   Dequantized Matrix A range: [{deq_a.min():.6f}, {deq_a.max():.6f}]")
    print(f"   Dequantized Matrix B range: [{deq_b.min():.6f}, {deq_b.max():.6f}]")

    # Define GEMM data types
    product_dtype = GFPDataType(
        mantissa_bits=19,
        exp_bits=8,
        mantissa_signed=True
    )
    accum_dtype = GFPDataType(
        mantissa_bits=20,
        exp_bits=9,
        mantissa_signed=True
    )

    # Run GFP GEMM - computes (B*V) x (C*V) GEMM
    gemm = GFPGemm(accum_dtype=accum_dtype, product_dtype=product_dtype)
    gfp_result = gemm(gfp_a, gfp_b)
    result_full = gfp_result.dequantize().cpu().numpy()  # Shape: (B*V) x (C*V)

    print(f"   Full GEMM result shape: {result_full.shape}")
    print(f"   Result range: [{result_full.min():.6f}, {result_full.max():.6f}]")

    # V-loop accumulation: sum over V dimension
    # result[b,c] = sum over v of result_full[b*V+v, c*V+v]
    result_accumulated = np.zeros((B, C), dtype=np.float32)
    for b in range(B):
        for c in range(C):
            for v in range(V):
                result_accumulated[b, c] += result_full[b*V + v, c*V + v]

    print(f"   Accumulated result shape: {result_accumulated.shape}")

    # Convert to FP16 hex
    result_fp16 = result_accumulated.astype(np.float16).flatten()
    results_emulator = np.array([np.frombuffer(val.tobytes(), dtype=np.uint16)[0] for val in result_fp16], dtype=np.uint16)

    print(f"   ✓ Generated {len(results_emulator)} FP16 results ({B}x{C})")
    print(f"   First 4 values: {' '.join(f'0x{v:04x}' for v in results_emulator[:min(4, len(results_emulator))])}")

    return results_emulator


def main():
    # Parse command-line arguments
    parser = argparse.ArgumentParser(description='Generate hardware-accurate GFP GEMM golden reference')
    parser.add_argument('--B', type=int, default=128, help='Output rows (batch size)')
    parser.add_argument('--C', type=int, default=128, help='Output columns')
    parser.add_argument('--V', type=int, default=1, help='Inner dimension multiplier (V Native Vectors)')
    parser.add_argument('--multitile', action='store_true', help='Generate multi-tile golden reference')
    args = parser.parse_args()
    
    B, C, V = args.B, args.C, args.V
    
    # Validate parameters
    if B * V > 128 or C * V > 128:
        print(f"ERROR: Invalid parameters! BxV={B*V} and CxV={C*V} must both be ≤ 128")
        sys.exit(1)
    
    # Handle multi-tile mode
    if args.multitile:
        print("=" * 80)
        print("Multi-Tile Golden Reference Generation")
        print("=" * 80)

        # Generate multi-tile golden reference
        flat_results, command_sequence = generate_multitile_golden_reference(B, C, V)

        print("\n" + "=" * 80)
        print("Multi-tile golden reference generation complete!")
        print("Files generated:")
        print(f"  golden_B{B}_C{C}_V{V}_multitile.hex - Flat FP16 results")
        print(f"  golden_commands_b{B}_c{C}_v{V}_multitile.txt - Command sequence")
        print("=" * 80)
        print("\n")

    # Single-tile mode (always runs, both standalone and after multi-tile)
    print("=" * 80)
    if args.multitile:
        print("Single-Tile Golden Reference Generation (for same B, C, V)")
    else:
        print("Hardware-Accurate GFP GEMM Reference Generation")
    print("=" * 80)
    print(f"\nConfiguration: B={B}, C={C}, V={V}")
    print(f"  Matrix A: {B} x {128*V} (uses {B*V} Native Vectors)")
    print(f"  Matrix B: {128*V} x {C} (uses {C*V} Native Vectors)")
    print(f"  Output:   {B} x {C} = {B*C} results")

    # Load matrices from hex files
    print("\n1. Loading matrices from hex files...")
    left_hex_path = os.path.join(script_dir, 'left.hex')
    right_hex_path = os.path.join(script_dir, 'right.hex')

    exp_left_raw, man_left_raw = load_hex_file(left_hex_path)
    left_exp_torch = decode_exponents(exp_left_raw)
    left_exp = left_exp_torch.numpy()
    left_mant = decode_mantissas(man_left_raw)

    exp_right_raw, man_right_raw = load_hex_file(right_hex_path)
    right_exp_torch = decode_exponents(exp_right_raw)
    right_exp = right_exp_torch.numpy()
    right_mant = decode_mantissas(man_right_raw)

    print(f"   Left matrix: {left_mant.shape}, exponents: {left_exp.shape}")
    print(f"   Right matrix: {right_mant.shape}, exponents: {right_exp.shape}")
    print(f"   Mantissa range: [{np.min(left_mant)}, {np.max(left_mant)}]")
    print(f"   Exponent range: [{np.min(left_exp)}, {np.max(left_exp)}]")

    # Compute using selected method(s)
    results_hardware = None
    results_emulator = None
    
    # Initialize hardware-accurate compute engine
    print("\n2. Computing with Hardware-Accurate Algorithm...")
    hw_compute = HardwareGFPCompute(exp_bits=5, exp_bias=15, group_size=32)
    print(f"   Computing {B}x{C} GEMM with V={V} accumulation...")
    results_hardware = hw_compute.compute_gemm_with_bcv(left_mant, left_exp, right_mant, right_exp, B, C, V)
    print(f"   ✓ Generated {len(results_hardware)} FP16 results ({B}x{C})")
    print(f"   First 4 values: {' '.join(f'0x{v:04x}' for v in results_hardware[:4])}")
    
    # Compute using GFP emulator for comparison (new simplified implementation)
    print("\n3. Computing with GFP Emulator (same matrices, emulator algorithm)...")
    results_emulator = compute_emulator_golden(B, C, V, left_mant, left_exp, right_mant, right_exp)
    
    # Compare hardware vs emulator results with comprehensive metrics
    if results_hardware is not None and results_emulator is not None:
        print("\n" + "=" * 80)
        print(f"Hardware vs Emulator Comparison (B={B}, C={C}, V={V})")
        print("=" * 80)

        # Convert to float for numerical comparison
        hw_float = np.array([np.frombuffer(struct.pack('<H', v), dtype=np.float16)[0] for v in results_hardware], dtype=np.float32)
        emu_float = np.array([np.frombuffer(struct.pack('<H', v), dtype=np.float16)[0] for v in results_emulator], dtype=np.float32)

        # Exact FP16 match count
        total = len(results_hardware)
        exact_matches = np.sum(results_hardware == results_emulator)
        match_rate = 100.0 * exact_matches / total

        print(f"Total results: {total} ({B}x{C})")
        print(f"Exact FP16 matches: {exact_matches}/{total} ({match_rate:.1f}%)")

        # Compute error metrics
        abs_errors = np.abs(hw_float - emu_float)
        rmse = np.sqrt(np.mean((hw_float - emu_float)**2))
        max_error = np.max(abs_errors)
        mean_error = np.mean(abs_errors)

        print(f"\nNumerical Error Analysis:")
        print(f"  RMSE:               {rmse:.6e}")
        print(f"  Max absolute error: {max_error:.6e}")
        print(f"  Mean absolute error: {mean_error:.6e}")

        # Show detailed mismatches
        if exact_matches < total:
            print(f"\nFirst 10 mismatches:")
            mismatch_count = 0
            for i in range(total):
                if results_hardware[i] != results_emulator[i]:
                    hw_val = hw_float[i]
                    emu_val = emu_float[i]
                    diff = abs(hw_val - emu_val)
                    rel_err = (diff / abs(emu_val) * 100) if emu_val != 0 else 0
                    print(f"  [{i:3d}] HW=0x{results_hardware[i]:04x} ({hw_val:10.6f})  "
                          f"EMU=0x{results_emulator[i]:04x} ({emu_val:10.6f})  "
                          f"diff={diff:.6e} rel_err={rel_err:.2f}%")
                    mismatch_count += 1
                    if mismatch_count >= 10:
                        break
        else:
            print("\n✓ Perfect match between hardware and emulator!")

        print("=" * 80)
    
    # Write to output files (use hardware results by default, or emulator if that's all we have)
    results_to_write = results_hardware if results_hardware is not None else results_emulator

    # Write emulator results to _NEW file for validation
    if results_emulator is not None:
        emu_golden_path_new = os.path.join(script_dir, f'golden_B{B}_C{C}_V{V}_emu_NEW.hex')
        print(f"\n   Writing NEW emulator results to {emu_golden_path_new}...")
        with open(emu_golden_path_new, 'w') as f:
            for val in results_emulator:
                f.write(f"{val:04x}\n")
        print(f"   ✓ Wrote {len(results_emulator)} values")
        print(f"   NOTE: Compare this with golden_B{B}_C{C}_V{V}_emu.hex from generate_nv_hex.py")
    
    if results_to_write is not None:
        output_path = os.path.join(script_dir, 'out.hex')
        golden_path = os.path.join(script_dir, f'golden_B{B}_C{C}_V{V}.hex')

        print(f"\n5. Writing results...")
        print(f"   Writing to {output_path}...")
        with open(output_path, 'w') as f:
            for val in results_to_write:
                f.write(f"{val:04x}\n")
        print(f"   ✓ Wrote {len(results_to_write)} values")
        
        print(f"   Writing to {golden_path}...")
        with open(golden_path, 'w') as f:
            for val in results_to_write:
                f.write(f"{val:04x}\n")
        print(f"   ✓ Wrote {len(results_to_write)} values")

        # Verify first results
        print(f"\n6. Verification:")
        num_to_show = min(16, len(results_to_write))
        print(f"   First {num_to_show} results:")
        for i in range(num_to_show):
            row = i // C
            col = i % C
            fp16_bits = results_to_write[i]
            fp16_val = np.frombuffer(struct.pack('<H', fp16_bits), dtype=np.float16)[0]
            print(f"     [{i:2d}] output[{row},{col}] = 0x{fp16_bits:04x} ({fp16_val:8.4f})")

    print("\n" + "=" * 80)
    if args.multitile:
        print("All Golden Reference Generation Complete!")
        print("\nMulti-tile files generated:")
        print(f"  golden_B{B}_C{C}_V{V}_multitile.hex - Flat FP16 results")
        print(f"  golden_B{B}_C{C}_V{V}_multitile_commands.txt - Command sequence")
        print("\nSingle-tile files generated:")
        print(f"  golden_B{B}_C{C}_V{V}.hex - Hardware-accurate reference")
        print(f"  golden_B{B}_C{C}_V{V}_emu_NEW.hex - Emulator reference")
        print(f"  out.hex - Same as hardware-accurate reference")
    else:
        print("Golden reference generation complete!")
    print("\nHardware-accurate GFP algorithm used:")
    print("  - Group-by-group integer accumulation")
    print("  - Exponent-aligned integer summation (matches gfp8_nv_dot.sv)")
    print("  - V-loop accumulation (matches gfp8_bcv_controller.sv)")
    print("  - FP16 conversion with overflow clamping")
    print("=" * 80)


if __name__ == '__main__':
    main()
